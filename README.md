# Machine Learning Resources
Collection of links, tutorials and courses related to machine learning for easy reference.

## Activation Functions

* [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
* [Mish: Self Regularized Non-Monotonic Activation Function](https://github.com/digantamisra98/Mish)
------------ 
## Loss Functions

* [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)

------------ 
## Regularization

* [L1 and L2 Regularization Methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)
* [Intuitions on L1 and L2 Regularisation](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261)
* [Elastic Net Regression](https://www.youtube.com/watch?v=1dKRdX9bfIo) - Video
------------ 

## Dimensionality Reduction

* [Multi-Dimension Scaling (MDS)](https://blog.paperspace.com/dimension-reduction-with-multi-dimension-scaling/)
* [Principal Component Analysis (PCA)](https://www.youtube.com/watch?v=FgakZw6K1QQ) - Video
* [Principal Component Analysis - Explained Visually](http://setosa.io/ev/principal-component-analysis/)
* [A One-Stop Shop for Principal Component Analysis](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)
* [StatQuest: t-SNE, Clearly Explained](https://www.youtube.com/watch?v=NEaUSP4YerM) - Video
* [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)
* [Understanding UMAP](https://pair-code.github.io/understanding-umap/)
------------ 
## Boosting Algorithms

* [AdaBoost Tutorial](https://mccormickml.com/2013/12/13/adaboost-tutorial/)
* [AdaBoost Classifier in Python](https://www.datacamp.com/community/tutorials/adaboost-classifier-python)
* [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
* [Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained](https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725)
* [CatBoost overview](https://www.kaggle.com/mitribunskiy/tutorial-catboost-overview)
* [What is XGBoost](https://www.kaggle.com/dansbecker/xgboost)
* [What is LightGBM, How to implement it? How to fine tune the parameters?](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7)
------------ 
## Bayesian Networks

* [Making Your Neural Network Say “I Don’t Know” — Bayesian NNs using Pyro and PyTorch](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd)
* [Bayesian deep learning with Fastai : how not to be uncertain about your uncertainty !](https://towardsdatascience.com/bayesian-deep-learning-with-fastai-how-not-to-be-uncertain-about-your-uncertainty-6a99d1aa686e)
* [Bayesian Methods for Machine Learning](https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml) - Coursera
------------ 

## Meta-Learning

* [Learning to learn aka Meta-Learning - Curriculum](https://dudeperf3ct.github.io/meta/learning/2019/04/29/Fun-of-Dissecting-Paper/)
* [MAML tensorflow implementation - Colab Notebook](https://colab.research.google.com/github/mari-linhares/tensorflow-maml/blob/master/maml.ipynb#scrollTo=6bCe8vojB2ad)
* [Model Agnostic Meta Learning](https://www.youtube.com/watch?v=wT45v8sIMDM) - Video
* [Explanation of One-shot Learning with Memory-Augmented Neural Networks](https://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html)
------------
## NLP

* [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)
* [Paper Dissected: “Attention is All You Need” Explained](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)
* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)
* [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https://www.youtube.com/watch?v=TQQlZhbC5ps) - Video
* [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
------------
## Normalization

* [Box-Cox Transformations](http://onlinestatbook.com/2/transformations/box-cox.html)
* [The Box-Cox Transformation](https://nickcdryan.com/2017/04/19/the-box-cox-transformation/)
------------ 
## GAN
* [Generative Adversarial Networks](https://www.youtube.com/playlist?list=PLdxQ7SoCLQAMGgQAIAcyRevM8VvygTpCu) - Video Playlist

------------ 
## Misc

* [Jupyter Notebook Extensions](https://towardsdatascience.com/jupyter-notebook-extensions-517fa69d2231)
* [PYRO - Deep Universal Probabilistic Programming](https://pyro.ai/)
* [Online/Incremental Learning with Keras and Creme](https://www.pyimagesearch.com/2019/06/17/online-incremental-learning-with-keras-and-creme/)

